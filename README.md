
**Jamia Hamdard University B.Tech Minor Project in Speech Emotions Recognition System using Deep Learning**

<h1 align="left">SPEECH EMOTION RECOGNITION SYSTEM</h1>
Through all the available senses humans can actually sense the emotional state of their communication partner. The emotional detection is natural for humans but it is very difficult task for computers; although they can easily understand content based information, accessing the depth behind content is difficult and thatâ€™s what speech emotion recognition (SER) sets out to do. It is a system through which various audio speech files are classified into different emotions such as happy, sad, anger and neutral by computer. SER can be used in areas such as the medical field or customer call centers.


<h2 align="left">DATASET</h2>
The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) Dataset from Kaggle contains 1440 audio files from 24 Actors vocalizing two lexically-matched statements. Emotions include angry, happy, sad, fearful, calm, neutral, disgust, and surprised.(Dataset)['https://www.kaggle.com/datasets/uwrfkaggler/ravdess-emotional-speech-audio']

<h2 align="left">STEPS</h2>


STEP 1: IMPORT DEPENDENCIES LIBRARIES

STEP 2: LOAD THE RAVDESS DATASET

STEP 3: USING SPEECH RECOGNITION API TO CONVERT AUDIO TO TEXT

STEP 4: PLOTTING visuals TO UNDERSTAND RAW AUDIO FILES

STEP 5: CLEANING & MASKING

STEP 6: FEATURE EXTRACTION

STEP 7: LABELS CLASSIFICATION

STEP 8: LOADING OF DATA & SPLITTING OF DATASET

STEP 9: APPLY MLP CLASSIFIER

STEP 10: SAVING THE MODEL
